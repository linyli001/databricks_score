{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use AML Pipeline for Databricks Deployment\n",
    "\n",
    "In this example we use the DatabricksSteps to construct an AML pipeline for user to deploy score script in Databricks.\n",
    "\n",
    "In this notebook you will learn how to:\n",
    " 1. Create Azure Machine Learning Workspace object.\n",
    " 2. Create a Databricks compute target.\n",
    " 3. Construct a DatabricksStep for the python score script in Databricks.\n",
    " 4. Submit and Publish an AML pipeline to deploy the score scipt.\n",
    "\n",
    "Before running this notebook:\n",
    " 1. please install the azureml-sdk for your conda enviroment firstly.\n",
    " 2. prepare the python score script for deployment. You need to write your own score script. And there is a simple example in https://github.com/linyli001/databricks_score/blob/master/score.py to reference. You can download the example score.py to your local path, or upload it to any dbfs path in your databricks.\n",
    "\n",
    "\n",
    "### Check the Azure ML Core SDK Version to Validate Your Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK version: 1.0.6\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import azureml.core\n",
    "from azureml.core.runconfig import JarLibrary\n",
    "from azureml.core.compute import ComputeTarget, DatabricksCompute\n",
    "from azureml.exceptions import ComputeTargetException\n",
    "from azureml.core import Workspace, Run, Experiment\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.pipeline.steps import DatabricksStep\n",
    "from azureml.core.datastore import Datastore\n",
    "from azureml.data.data_reference import DataReference\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "# Initialize an Azure ML Workspace\n",
    "\n",
    "An Azure ML workspace is an Azure resource that organizes and coordinates the actions of many other Azure resources to assist in executing and sharing machine learning workflows. To create or access an Azure ML workspace, you will need to import the Azure ML library and specify following information:\n",
    "  1. **workspace_name** - A name for your workspace. You can choose one.\n",
    "  2. **subscription_id** - Your subscription id. Use the id value from the az account show command output above.\n",
    "  3. **resource_group** - The resource group name. The resource group organizes Azure resources and provides a default region for the resources in the group. The resource group will be created if it doesn't exist. Resource groups can be created and viewed in the Azure portal \n",
    "  4. **workspace_region** - Supported regions include eastus2, eastus,westcentralus, southeastasia, westeurope, australiaeast, westus2, southcentralus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_id = \"<Your SubscriptionId>\" #you should be owner or contributor\n",
    "resource_group = \"<Resource group - new or existing>\" #you should be owner or contributor\n",
    "workspace_name = \"<workspace to be created>\" #your workspace name\n",
    "workspace_region = \"<azureregion>\" #your region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Workspace class and check the Azure ML SDK version.\n",
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.create(name = workspace_name,\n",
    "                      subscription_id = subscription_id,\n",
    "                      resource_group = resource_group, \n",
    "                      location = workspace_region,                      \n",
    "                      exist_ok=True)\n",
    "ws.get_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace(workspace_name = workspace_name,\n",
    "               subscription_id = subscription_id,\n",
    "               resource_group = resource_group)\n",
    "\n",
    "# Persist the subscription id, resource group name, and workspace name in aml_config/config.json.\n",
    "ws.write_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attach Databricks compute target\n",
    "Next, you need to add your Databricks workspace to Azure Machine Learning as a compute target and give it a name. You will use this name to refer to your Databricks workspace compute target inside Azure Machine Learning.\n",
    "1. **Compute Target** - Any name you given to tag the compute target\n",
    "2. **Resource Group** - The resource group name of your Databricks workspace  \n",
    "3. **Databricks Workspace Name** - The workspace name of your Databricks workspace  \n",
    "4. **Databricks Access Token** - The access token you created in ADB\n",
    "\n",
    "**The Databricks workspace need to be present in the same subscription as your AML workspace**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_compute_name = \"<DATABRICKS_COMPUTE_NAME>\"  # Databricks compute name\n",
    "db_resource_group = \"<DATABRICKS_RESOURCE_GROUP>\" # Databricks resource group\n",
    "db_workspace_name = \"<DATABRICKS_WORKSPACE_NAME>\" # Databricks workspace name\n",
    "db_access_token = \"<DATABRICKS_ACCESS_TOKEN>\" # Databricks access token\n",
    " \n",
    "try:\n",
    "    databricks_compute = ComputeTarget(workspace=ws, name=db_compute_name)\n",
    "    print('Compute target {} already exists'.format(db_compute_name))\n",
    "except ComputeTargetException:\n",
    "    print('Compute not found, will use below parameters to attach new one')\n",
    "    print('db_compute_name {}'.format(db_compute_name))\n",
    "    print('db_resource_group {}'.format(db_resource_group))\n",
    "    print('db_workspace_name {}'.format(db_workspace_name))\n",
    "    print('db_access_token {}'.format(db_access_token))\n",
    " \n",
    "    config = DatabricksCompute.attach_configuration(\n",
    "        resource_group = db_resource_group,\n",
    "        workspace_name = db_workspace_name,\n",
    "        access_token= db_access_token)\n",
    "    databricks_compute=ComputeTarget.attach(ws, db_compute_name, config)\n",
    "    databricks_compute.wait_for_completion(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Databricks from Azure Machine Learning Pipeline\n",
    "\n",
    "To use Databricks as a compute target from Azure Machine Learning Pipeline, a DatabricksStep is used. DatabricksStep supports to run the score file for notebook, python or jar. To learn more about the paramters please reference https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-databricks-as-compute-target.ipynb, here we use the python script in this example.\n",
    "\n",
    "Now you can create your own score.py in local compute path, or just download the score.py from given github. Be attention that to use the example score.py, you need to firstly upload your test data as well as prepared model to your Azure blob storage, and then score.py can help to load the data and model for prediction.\n",
    "\n",
    "Below is the config parameters to run the example score.py in local path.\n",
    "\n",
    "1. **databricks_step_name** - name of this databricks step, which is necessary.\n",
    "2. **spark_version, node_type, num_workers, spark_env_variables** - the configs to create a new cluster in databricks, if using the existing one, use **existing_cluster_id** to replace.\n",
    "3. **python_script_name** - for local script, this is the script name(relative to source_directory). In this case, it's \"score.py\".\n",
    "4. **source_directory** - the local storage path for python script.\n",
    "5. **databricks_compute** - Databricks target we just created.\n",
    "6. **run_name**: Name in databricks for this run  \n",
    "\n",
    "**PS**: **python_script_params** is the command line to run the python script. Below is the run parameters necessary for the example score.py. If use customized score, the paramters will change.\n",
    "\n",
    "1. **asb_account** - Azure Blob account, can be wasbs://[your-container-name]@[your-storage-account-name].blob.core.windows.net/[your-directory-name]. This is the Azure blob holding test data and models.\n",
    "2. **asb_key_name** - Azure Blob key config, can be fs.azure.account.key.[your-storage-account-name].blob.core.windows.net\n",
    "3. **asb_key** - Private key to access your blob.\n",
    "4. **data_path** - Relative path to your directory where test data is stored in Azure blob. Must be start without \"/\" \n",
    "5. **data_path** - Relative path to your directory where model is stored in Azure blob. Must be start without \"/\". And model format is PipelineModel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### run python script example score.py in local machine path \n",
    "\n",
    "dbNbStep = DatabricksStep(\n",
    "    name = \"<Databricks Step Name>\",\n",
    "    spark_version = \"<Spark version>\",\n",
    "    node_type = \"<Node Type>\",\n",
    "    num_workers = \"<Num Workers>\",\n",
    "    spark_env_variables = {'PYSPARK_PYTHON': '/databricks/python3/bin/python3'},\n",
    "    python_script_name = \"score.py\",\n",
    "    source_directory = \"<Local Script Path>\",\n",
    "    python_script_params = [\"--asb_account\", \"wasbs://<your-container-name>@<your-storage-account-name>.blob.core.windows.net/<your-directory-name>\",\n",
    "                          \"--asb_key_name\", \"fs.azure.account.key.<your-storage-account-name>.blob.core.windows.net\",\n",
    "                          \"--asb_key\", \"<Private Key to Azure Blob>\",\n",
    "                          \"--data_path\", \"<Relative data Path to Blob Directory>\",\n",
    "                          \"--model_path\", \"<Relative model Path to Blob Directory>\"\n",
    "                         ],\n",
    "    run_name = \"<Run name for Databricks>\",\n",
    "    compute_target = databricks_compute,\n",
    "    allow_reuse = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Construct the pipeline with create DatabricksStep and submit Pipeline   \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [dbNbStep]\n",
    "pipeline = Pipeline(workspace=ws, steps=steps)\n",
    "pipeline_run = Experiment(ws, 'db_run_score').submit(pipeline)\n",
    "pipeline_run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the Run Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_runfrom azureml.widgets import RunDetails\n",
    "RunDetails(pipeline_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Publish the pipeline\n",
    "\n",
    "After you're satisfied with the outcome of the run, publish the pipeline so you can run it with different input values later. When you publish a pipeline, you get a REST endpoint. This endpoint accepts invoking of the pipeline with the set of parameters you have already incorporated by using PipelineParameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline = pipeline_run.publish_pipeline(\n",
    "    name=\"databricks_scoring\", \n",
    "    description=\"Batch scoring using pipeline model\", \n",
    "    version=\"1.0\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
